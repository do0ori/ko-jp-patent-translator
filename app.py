import streamlit as st
import pandas as pd
from docx import Document
from docx.oxml.ns import qn
from docx.shared import Pt
from PIL import Image
from io import BytesIO
from google import genai
import os
from pydantic import BaseModel
from typing import List
import tempfile
from pathlib import Path
from datetime import datetime

# Gemini API ì„¤ì •
client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])

# êµ¬ì¡°í™” ëª¨ë¸
class TranslationResult(BaseModel):
    translated_text: str

class ImageTranslation(BaseModel):
    original: str
    translated: str

# í…ìŠ¤íŠ¸ ë²ˆì—­
def translate_text_with_gemini(text: str) -> str:
    prompt = (
        "Translate the following Korean patent document text into Japanese. "
        "Translate it naturally, but maintain technical and structural fidelity. "
        "Translate domain-specific technical terms with reference to official or trusted Japanese sources."
    )
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[prompt, text],
        config={
            "response_mime_type": "application/json",
            "response_schema": TranslationResult,
        },
    )
    result: TranslationResult = response.parsed
    return result.translated_text

# ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ë²ˆì—­
def translate_image_with_gemini(pil_image) -> list[ImageTranslation]:
    prompt = (
        "Extract all visible Korean or English text from this image, and translate each into Japanese. "
        "Format the result as a list of objects with 'original' and 'translated' keys."
    )
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[prompt, pil_image],
        config={
            "response_mime_type": "application/json",
            "response_schema": list[ImageTranslation],
        },
    )
    return response.parsed


# ì´ˆê¸° ìƒíƒœ
if "translated" not in st.session_state:
    st.session_state.translated = False
if "output_path" not in st.session_state:
    st.session_state.output_path = None
if "parsed_elements" not in st.session_state:
    st.session_state.parsed_elements = []
if "chunked_elements" not in st.session_state:
    st.session_state.chunked_elements = []

# í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
st.set_page_config(page_title="í•œì¼ íŠ¹í—ˆ ë²ˆì—­ê¸°", layout="centered")
st.title("ğŸ“„ í•œì¼ íŠ¹í—ˆ ë²ˆì—­ê¸°")
st.markdown("ì—…ë¡œë“œí•œ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ìë™ ë¶„ì„í•˜ì—¬ Ai ê¸°ë°˜ìœ¼ë¡œ í•œì¼ ë²ˆì—­ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“¤ ë²ˆì—­í•  .docx íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["docx"])

# ì§„í–‰ë¥  í‘œì‹œ ìœ„ì¹˜ í™•ë³´
progress_placeholder = st.empty()

# ë¬¸ì„œ íŒŒì‹± í•¨ìˆ˜
def parse_docx_with_images(docx_file):
    doc = Document(docx_file)
    elements = []

    rels = doc.part._rels
    image_map = {}

    for rel in rels:
        rel_obj = rels[rel]
        if "image" in rel_obj.reltype:
            image_bytes = rel_obj.target_part.blob
            image = Image.open(BytesIO(image_bytes))
            image_map[rel_obj.rId] = image

    for para in doc.paragraphs:
        text = para.text.strip()
        if text:
            elements.append({"type": "TEXT", "content": text})
        for run in para.runs:
            drawing = run._element.find(
                ".//w:drawing", namespaces={"w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main"}
            )
            if drawing is not None:
                blip = drawing.find(".//a:blip", namespaces={"a": "http://schemas.openxmlformats.org/drawingml/2006/main"})
                if blip is not None:
                    embed_id = blip.get("{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed")
                    if embed_id in image_map:
                        elements.append({"type": "FIGURE", "content": image_map[embed_id]})
    return elements

# ë‹¨ë½ë“¤ì„ ë¬¶ì–´ì„œ chunk ìƒì„±
def group_paragraphs_to_chunks(elements, max_words=200):
    chunks, buffer, word_count = [], [], 0
    for elem in elements:
        if elem["type"] == "TEXT":
            words = len(elem["content"].split())
            if word_count + words > max_words and buffer:
                chunks.append({"type": "TEXT", "content": "\n".join(buffer)})
                buffer, word_count = [], 0
            buffer.append(elem["content"])
            word_count += words
        elif elem["type"] == "FIGURE":
            if buffer:
                chunks.append({"type": "TEXT", "content": "\n".join(buffer)})
                buffer, word_count = [], 0
            chunks.append(elem)
    if buffer:
        chunks.append({"type": "TEXT", "content": "\n".join(buffer)})
    return chunks

# ì¶œë ¥ìš© docx ì´ˆê¸°í™” í•¨ìˆ˜
def create_japanese_patent_docx():
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'MS Mincho'
    font.size = Pt(10.5)
    style._element.rPr.rFonts.set(qn('w:eastAsia'), 'ï¼­ï¼³ æ˜æœ')
    return doc

# ë²ˆì—­ ì‹¤í–‰
def run_translation():
    doc = create_japanese_patent_docx()
    total = len(st.session_state.chunked_elements)

    for i, chunk in enumerate(st.session_state.chunked_elements):
        if chunk["type"] == "TEXT":
            translated = translate_text_with_gemini(chunk["content"])
            doc.add_paragraph(translated)
            chunk["translated"] = translated
        elif chunk["type"] == "FIGURE":
            translated_pairs = translate_image_with_gemini(chunk["content"])
            formatted = [f'{p.original}: {p.translated}' for p in translated_pairs]
            for line in formatted:
                doc.add_paragraph(line)
            chunk["translated"] = formatted

        progress_placeholder.progress((i + 1) / total, text=f"ğŸ”„ ë²ˆì—­ ì¤‘... {i + 1} / {total} ì²­í¬ ì™„ë£Œ")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_file:
        doc.save(tmp_file.name)
        st.session_state.output_path = tmp_file.name
    
    st.session_state.translated = True

# íŒŒì¼ ì œê±° or ë³€ê²½ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
if uploaded_file is None:
    st.session_state.translated = False
    st.session_state.output_path = None
    st.session_state.parsed_elements = []
    st.session_state.chunked_elements = []
    st.session_state.base_filename = ""
else:
    new_filename = uploaded_file.name
    if st.session_state.get("last_uploaded_filename") != new_filename:
        st.session_state.translated = False
        st.session_state.output_path = None
        st.session_state.parsed_elements = []
        st.session_state.chunked_elements = []
        st.session_state.last_uploaded_filename = new_filename
        st.session_state.base_filename = Path(new_filename).stem

# íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë¬¸ì„œ íŒŒì‹±
if uploaded_file and not st.session_state.translated:
    elements = parse_docx_with_images(uploaded_file)
    chunks = group_paragraphs_to_chunks(elements, max_words=200)
    st.session_state.parsed_elements = elements
    st.session_state.chunked_elements = chunks

# ë²ˆì—­ ì‹œì‘ ë²„íŠ¼
if uploaded_file and not st.session_state.translated:
    st.button("ğŸš€ ë²ˆì—­ ì‹œì‘", on_click=run_translation)

# ë²ˆì—­ ì™„ë£Œ í›„ ê²°ê³¼
if st.session_state.translated:
    st.success("âœ… ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    download_filename = f"{st.session_state.base_filename}_translated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
    
    with open(st.session_state.output_path, "rb") as f:
        st.download_button(
            label="ğŸ“¥ ë²ˆì—­ëœ .docx ë‹¤ìš´ë¡œë“œ",
            data=f,
            file_name=download_filename,
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )

    # ë²ˆì—­ ê²°ê³¼ í‘œì‹œ
    with st.expander("ğŸ“˜ ìµœì¢… ë²ˆì—­ ê²°ê³¼ (ì²­í¬ ë‹¨ìœ„)", expanded=False):
        df = pd.DataFrame(st.session_state.chunked_elements)
        st.dataframe(df, use_container_width=True)
