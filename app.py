import tempfile
from datetime import datetime
from pathlib import Path

import pandas as pd
import streamlit as st

from utils import (
    create_japanese_patent_docx,
    group_paragraphs_to_chunks,
    parse_docx_with_images,
    translate_image_with_gemini,
    translate_text_with_gemini,
)
from utils.models import get_model_list, get_default_model

# ì´ˆê¸° ìƒíƒœ
if "translated" not in st.session_state:
    st.session_state.translated = False
if "output_path" not in st.session_state:
    st.session_state.output_path = None
if "parsed_elements" not in st.session_state:
    st.session_state.parsed_elements = []
if "chunked_elements" not in st.session_state:
    st.session_state.chunked_elements = []
if "selected_model" not in st.session_state:
    st.session_state.selected_model = get_default_model()

# í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
st.set_page_config(page_title="í•œì¼ íŠ¹í—ˆ ë²ˆì—­ê¸°", layout="centered")
st.title("ğŸ“„ í•œì¼ íŠ¹í—ˆ ë²ˆì—­ê¸°")
st.markdown(
    "ì—…ë¡œë“œí•œ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ìë™ ë¶„ì„í•˜ì—¬ AI ê¸°ë°˜ìœ¼ë¡œ í•œì¼ ë²ˆì—­ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
)

# ëª¨ë¸ ì„ íƒ ì„¹ì…˜
models = get_model_list()

# ëª¨ë¸ ì„ íƒ ì˜µì…˜ ìƒì„±
model_options = []
for model_id, model_info in models.items():
    display_name = model_info["name"]
    if model_info.get("recommended", False):
        display_name += " â­ (ì¶”ì²œ)"
    model_options.append((model_id, display_name))

# ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
selected_model_id = st.selectbox(
    "ğŸ¤– ë²ˆì—­ì— ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
    options=[opt[0] for opt in model_options],
    index=[opt[0] for opt in model_options].index(st.session_state.selected_model),
    format_func=lambda x: next(opt[1] for opt in model_options if opt[0] == x)
)

# ì„ íƒëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
if selected_model_id in models:
    model_info = models[selected_model_id]
    st.info(f"**ì„ íƒëœ ëª¨ë¸:** {model_info['name']}\n\n**ì„¤ëª…:** {model_info['description']}")

# ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
st.session_state.selected_model = selected_model_id

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“¤ ë²ˆì—­í•  .docx íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["docx"])

# ì§„í–‰ë¥  í‘œì‹œ ìœ„ì¹˜ í™•ë³´
progress_placeholder = st.empty()

# íŒŒì¼ ì œê±° or ë³€ê²½ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
if uploaded_file is None:
    st.session_state.translated = False
    st.session_state.output_path = None
    st.session_state.parsed_elements = []
    st.session_state.chunked_elements = []
    st.session_state.base_filename = ""
else:
    new_filename = uploaded_file.name
    if st.session_state.get("last_uploaded_filename") != new_filename:
        st.session_state.translated = False
        st.session_state.output_path = None
        st.session_state.parsed_elements = []
        st.session_state.chunked_elements = []
        st.session_state.last_uploaded_filename = new_filename
        st.session_state.base_filename = Path(new_filename).stem

# íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë¬¸ì„œ íŒŒì‹±
if uploaded_file and not st.session_state.translated:
    elements = parse_docx_with_images(uploaded_file)
    chunks = group_paragraphs_to_chunks(elements)
    st.session_state.parsed_elements = elements
    st.session_state.chunked_elements = chunks


# ë²ˆì—­ ì‹¤í–‰
def run_translation():
    doc = create_japanese_patent_docx()
    total = len(st.session_state.chunked_elements)
    paragraph_counter = 0

    for i, chunk in enumerate(st.session_state.chunked_elements):
        if chunk["type"] == "TEXT":
            translated = translate_text_with_gemini(chunk["content"], st.session_state.selected_model)
            for line in translated.split("\n"):
                if line.strip():
                    # ì œëª©ì€ ë‹¨ë½ ë²ˆí˜¸ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ -> ì œëª©ì´ ì•„ë‹Œ lineì— ëŒ€í•´ ë‹¨ë½ ë²ˆí˜¸ ì¶”ê°€
                    if not (line.startswith("ã€") and line.endswith("ã€‘")):
                        # ì²« ë²ˆì§¸ ë‹¨ë½ì€ íŠ¹í—ˆ ëª…ì¹­ì— ëŒ€í•œ ë‹¨ë½ì´ë¯€ë¡œ ë²ˆí˜¸ë¥¼ ë¶™ì´ì§€ ì•ŠìŒ
                        if paragraph_counter == 0:
                            paragraph_counter += 1
                        else:
                            paragraph_number = f" ã€{paragraph_counter:04d}ã€‘"
                            paragraph_counter += 1
                            doc.add_paragraph_with_justify(" " + paragraph_number)
                    doc.add_paragraph_with_justify(" " + line)
                else:
                    doc.add_paragraph_with_justify("")
            chunk["translated"] = translated
        elif chunk["type"] == "FIGURE":
            translated_pairs = translate_image_with_gemini(chunk["content"], st.session_state.selected_model)
            formatted = [f"{p.original}: {p.translated}" for p in translated_pairs]
            for line in formatted:
                doc.add_paragraph_with_justify(line)
            chunk["translated"] = formatted

        progress_placeholder.progress(
            (i + 1) / total, text=f"ğŸ”„ ë²ˆì—­ ì¤‘... {i + 1} / {total} ì²­í¬ ì™„ë£Œ"
        )

    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp_file:
        doc.save(tmp_file.name)
        st.session_state.output_path = tmp_file.name

    st.session_state.translated = True


# ë²ˆì—­ ì‹œì‘ ë²„íŠ¼
if uploaded_file and not st.session_state.translated:
    st.button("ğŸš€ ë²ˆì—­ ì‹œì‘", on_click=run_translation)

# ë²ˆì—­ ì™„ë£Œ í›„ ê²°ê³¼
if st.session_state.translated:
    # ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ
    used_model_info = models.get(st.session_state.selected_model, {})
    st.success(f"âœ… ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (ì‚¬ìš© ëª¨ë¸: {used_model_info.get('name', st.session_state.selected_model)})")

    download_filename = f"{st.session_state.base_filename}_translated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"

    with open(st.session_state.output_path, "rb") as f:
        st.download_button(
            label="ğŸ“¥ ë²ˆì—­ëœ .docx ë‹¤ìš´ë¡œë“œ",
            data=f,
            file_name=download_filename,
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

    # ë²ˆì—­ ê²°ê³¼ í‘œì‹œ
    with st.expander("ğŸ“˜ ìµœì¢… ë²ˆì—­ ê²°ê³¼ (ì²­í¬ ë‹¨ìœ„)", expanded=False):
        df = pd.DataFrame(st.session_state.chunked_elements)
        st.dataframe(df, use_container_width=True)
